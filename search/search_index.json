{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"home/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Tutorials"},{"location":"home/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"home/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"home/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"documents/boml.init_optimize/","text":"class Init Description Complete Meta-learning Process with MAML and MAML-based methods Implements the meta learning procedure of MAML [1] and four MAML based methods, Meta-SGD [2] , MT-net [3] , Warp-grad [4] and L2F [5] . Parameters model : Module Model containing backbone network and other auxiliary meta modules if using other MAML-based methods. inner_objective : callable The inner loop optimization objective. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of inner objective. The state object contains the following: \"data\"(Tensor) - Data used in inner optimization phase. \"target\"(Tensor) - Target used in inner optimization phase. \"model\"(Module) - Meta model to be updated. \"updated_weights\"(List[Parameter]) - Weights of model updated in inner-loop, will be used for forward propagation. outer_objective : callable The outer optimization objective. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of outer objective. The state object contains the following: \"data\"(Tensor) - Data used in outer optimization phase. \"target\"(Tensor) - Target used in outer optimization phase. \"model\"(Module) - Meta model to be updated. \"updated_weights\"(List[Parameter]) - Weights of model updated in inner-loop, will be used for forward propagation. inner_learning_rate : float, default=0.01 Step size for inner optimization. inner_loop : int, default=5 Num of inner optimization steps. use_second_order (optional): bool, default=True Optional argument,whether to calculate precise second-order gradients during inner-loop. learn_lr (optional): bool, default=False Optional argument, whether to update inner learning rate during outer optimization, i.e. use MSGD method. use_t (optional): bool, default=False Optional argument, whether to using T-layers during optimization,i.e. use MT-net method. use_warp (optional): bool, default=False Optional argument, whether to using warp modules during optimization,i.e. use Warp-grad method. use_forget (optional): bool, default=False Optional argument, whether to add attenuation to each layers, i.e. use L2F method. Methods optimize() The meta optimization process containing both inner loop phase and outer loop phase. Final grads will be calculated by outer objective and saved in the passed in model. Note that the implemented optimization procedure will compute the grads of meta model with only one single set of training and validation data samples in a batch. If batch size is larger than 1, then optimize() function should be called repeatedly to accumulate the grads of model variables for the whole batch. After that the update operation of model variable needs to be done outside this optimization module. Parameters: * train_data(Tensor) - The training data used in inner loop phase. train_target(Tensor) - The labels of the samples in the train data. validate_data(Tensor) - The validation data used in outer loop phase. validate_target(Tensor) - The labels of the samples in the validation data. Returns val_loss(Tensor) - The value of validation loss. References [1] C. Finn, P. Abbeel, S. Levine, \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\", in ICML, 2017. [2] Z. Li, F. Zhou, F. Chen, H. Li, \"Meta-SGD: Learning to Learn Quickly for Few-Shot Learning\", in arxiv, 2017. [3] Y. Lee and S. Choi, \"Gradient-Based Meta-Learning with Learned Layer-wise Metric and Subspace\", in ICML, 2018. [4] S. Flennerhag, A. Rusu, R. Pascanu, F. Visin, H. Yin, R. Hadsell, \"Meta-learning with Warped Gradient Descent\", in ICLR, 2020. [5] S. Baik, S. Hong, K. Lee, \"Learning to Forget for Meta-Learning\", in CVPR, 2020.","title":"both.meta_optimize"},{"location":"documents/boml.init_optimize/#classinit","text":"","title":"class&nbsp;&nbsp;Init"},{"location":"documents/boml.init_optimize/#description","text":"Complete Meta-learning Process with MAML and MAML-based methods Implements the meta learning procedure of MAML [1] and four MAML based methods, Meta-SGD [2] , MT-net [3] , Warp-grad [4] and L2F [5] .","title":"Description"},{"location":"documents/boml.init_optimize/#parameters","text":"model : Module Model containing backbone network and other auxiliary meta modules if using other MAML-based methods. inner_objective : callable The inner loop optimization objective. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of inner objective. The state object contains the following: \"data\"(Tensor) - Data used in inner optimization phase. \"target\"(Tensor) - Target used in inner optimization phase. \"model\"(Module) - Meta model to be updated. \"updated_weights\"(List[Parameter]) - Weights of model updated in inner-loop, will be used for forward propagation. outer_objective : callable The outer optimization objective. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of outer objective. The state object contains the following: \"data\"(Tensor) - Data used in outer optimization phase. \"target\"(Tensor) - Target used in outer optimization phase. \"model\"(Module) - Meta model to be updated. \"updated_weights\"(List[Parameter]) - Weights of model updated in inner-loop, will be used for forward propagation. inner_learning_rate : float, default=0.01 Step size for inner optimization. inner_loop : int, default=5 Num of inner optimization steps. use_second_order (optional): bool, default=True Optional argument,whether to calculate precise second-order gradients during inner-loop. learn_lr (optional): bool, default=False Optional argument, whether to update inner learning rate during outer optimization, i.e. use MSGD method. use_t (optional): bool, default=False Optional argument, whether to using T-layers during optimization,i.e. use MT-net method. use_warp (optional): bool, default=False Optional argument, whether to using warp modules during optimization,i.e. use Warp-grad method. use_forget (optional): bool, default=False Optional argument, whether to add attenuation to each layers, i.e. use L2F method.","title":"Parameters"},{"location":"documents/boml.init_optimize/#methods","text":"optimize() The meta optimization process containing both inner loop phase and outer loop phase. Final grads will be calculated by outer objective and saved in the passed in model. Note that the implemented optimization procedure will compute the grads of meta model with only one single set of training and validation data samples in a batch. If batch size is larger than 1, then optimize() function should be called repeatedly to accumulate the grads of model variables for the whole batch. After that the update operation of model variable needs to be done outside this optimization module. Parameters: * train_data(Tensor) - The training data used in inner loop phase. train_target(Tensor) - The labels of the samples in the train data. validate_data(Tensor) - The validation data used in outer loop phase. validate_target(Tensor) - The labels of the samples in the validation data. Returns val_loss(Tensor) - The value of validation loss.","title":"Methods"},{"location":"documents/boml.init_optimize/#references","text":"[1] C. Finn, P. Abbeel, S. Levine, \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\", in ICML, 2017. [2] Z. Li, F. Zhou, F. Chen, H. Li, \"Meta-SGD: Learning to Learn Quickly for Few-Shot Learning\", in arxiv, 2017. [3] Y. Lee and S. Choi, \"Gradient-Based Meta-Learning with Learned Layer-wise Metric and Subspace\", in ICML, 2018. [4] S. Flennerhag, A. Rusu, R. Pascanu, F. Visin, H. Yin, R. Hadsell, \"Meta-learning with Warped Gradient Descent\", in ICLR, 2020. [5] S. Baik, S. Hong, K. Lee, \"Learning to Forget for Meta-Learning\", in CVPR, 2020.","title":"References"},{"location":"documents/boml/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. \\left\\{ \\begin{array}{l} 0 = c_x-a_{x0}-d_{x0}\\dfrac{(c_x-a_{x0})\\cdot d_{x0}}{\\|d_{x0}\\|^2} + c_x-a_{x1}-d_{x1}\\dfrac{(c_x-a_{x1})\\cdot d_{x1}}{\\|d_{x1}\\|^2} \\\\[2ex] 0 = c_y-a_{y0}-d_{y0}\\dfrac{(c_y-a_{y0})\\cdot d_{y0}}{\\|d_{y0}\\|^2} + c_y-a_{y1}-d_{y1}\\dfrac{(c_y-a_{y1})\\cdot d_{y1}}{\\|d_{y1}\\|^2} \\end{array} \\right. \\left\\{ \\begin{array}{l} 0 = c_x-a_{x0}-d_{x0}\\dfrac{(c_x-a_{x0})\\cdot d_{x0}}{\\|d_{x0}\\|^2} + c_x-a_{x1}-d_{x1}\\dfrac{(c_x-a_{x1})\\cdot d_{x1}}{\\|d_{x1}\\|^2} \\\\[2ex] 0 = c_y-a_{y0}-d_{y0}\\dfrac{(c_y-a_{y0})\\cdot d_{y0}}{\\|d_{y0}\\|^2} + c_y-a_{y1}-d_{y1}\\dfrac{(c_y-a_{y1})\\cdot d_{y1}}{\\|d_{y1}\\|^2} \\end{array} \\right. \\left{ \\begin{array}{l} 0 = c_x-a_{x0}-d_{x0}\\dfrac{(c_x-a_{x0})\\cdot d_{x0}}{|d_{x0}|^2} + c_x-a_{x1}-d_{x1}\\dfrac{(c_x-a_{x1})\\cdot d_{x1}}{|d_{x1}|^2} \\[2ex] 0 = c_y-a_{y0}-d_{y0}\\dfrac{(c_y-a_{y0})\\cdot d_{y0}}{|d_{y0}|^2} + c_y-a_{y1}-d_{y1}\\dfrac{(c_y-a_{y1})\\cdot d_{y1}}{|d_{y1}|^2} \\end{array} \\right.","title":"both"},{"location":"documents/boml/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"documents/boml/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"documents/boml/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. \\left\\{ \\begin{array}{l} 0 = c_x-a_{x0}-d_{x0}\\dfrac{(c_x-a_{x0})\\cdot d_{x0}}{\\|d_{x0}\\|^2} + c_x-a_{x1}-d_{x1}\\dfrac{(c_x-a_{x1})\\cdot d_{x1}}{\\|d_{x1}\\|^2} \\\\[2ex] 0 = c_y-a_{y0}-d_{y0}\\dfrac{(c_y-a_{y0})\\cdot d_{y0}}{\\|d_{y0}\\|^2} + c_y-a_{y1}-d_{y1}\\dfrac{(c_y-a_{y1})\\cdot d_{y1}}{\\|d_{y1}\\|^2} \\end{array} \\right. \\left\\{ \\begin{array}{l} 0 = c_x-a_{x0}-d_{x0}\\dfrac{(c_x-a_{x0})\\cdot d_{x0}}{\\|d_{x0}\\|^2} + c_x-a_{x1}-d_{x1}\\dfrac{(c_x-a_{x1})\\cdot d_{x1}}{\\|d_{x1}\\|^2} \\\\[2ex] 0 = c_y-a_{y0}-d_{y0}\\dfrac{(c_y-a_{y0})\\cdot d_{y0}}{\\|d_{y0}\\|^2} + c_y-a_{y1}-d_{y1}\\dfrac{(c_y-a_{y1})\\cdot d_{y1}}{\\|d_{y1}\\|^2} \\end{array} \\right. \\left{ \\begin{array}{l} 0 = c_x-a_{x0}-d_{x0}\\dfrac{(c_x-a_{x0})\\cdot d_{x0}}{|d_{x0}|^2} + c_x-a_{x1}-d_{x1}\\dfrac{(c_x-a_{x1})\\cdot d_{x1}}{|d_{x1}|^2} \\[2ex] 0 = c_y-a_{y0}-d_{y0}\\dfrac{(c_y-a_{y0})\\cdot d_{y0}}{|d_{y0}|^2} + c_y-a_{y1}-d_{y1}\\dfrac{(c_y-a_{y1})\\cdot d_{y1}}{|d_{y1}|^2} \\end{array} \\right.","title":"Project layout"},{"location":"documents/boml.optimizer/","text":"class BOTHOptimizer Description Wrapper for performing bi-level optimization and gradient-based initialization optimization BOTHOptimizer is the wrapper of Bi-Level Optimization(BLO) and Initialization Optimization(Initialization-based EGBR) process which builds LL, UL and Initialization problem solver with corresponding method modules and uses in training phase. The optimization process could also be done by using methods packages directly. Parameters method : str Define basic method for following training process, it should be included in ['MetaInit', 'MetaRepr']. 'MetaInit' type refers to meta-learning optimization strategy, including methods like 'MAML, FOMAML, TNet, WarpGrad, L2F'; 'MetaRepr' type refers to bi-level optimization strategy, includes methods like 'BDA, RHG, Truncated RHG, Implicit HG, Onestage, BVFIM, IAPTT-GM, BSG'. lower_method : str, default=None method chosen for solving LL problem, including ['Feature' ,'BVFIM']. upper_method : str, default=None Method chosen for solving UL problem, including ['Trad' ,'IAPTTGM', 'Implicit', 'BSG', 'Onestage', 'BVFIM']. lower_objective : callable, default=None An optimization problem which is considered as the constraint of UL problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\" Data used in the LL optimization phase. \"target\" Target used in the LL optimization phase. \"upper_model\" UL model of the bi-level model structure. \"lower_model\" LL model of the bi-level model structure. upper_objective : callable, default=None The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\" Data used in the UL optimization phase. \"target\" Target used in the UL optimization phase. \"upper_model\" Ul model of the bi-level model structure. \"lower_model\" LL model of the bi-level model structure. inner_objective : callable, default=None The inner loop optimization objective. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of inner objective. The state object contains the following: \"data\" Data used in inner optimization phase. \"target\" Target used in inner optimization phase. \"model\" Meta model to be updated. \"updated_weights\" Weights of model updated in inner-loop, will be used for forward propagation. outer_objective : callable, default=None The outer optimization objective. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of outer objective. The state object contains the following: \"data\" Data used in outer optimization phase. \"target\" Target used in outer optimization phase. \"model\" Meta model to be updated. \"updated_weights\" Weights of model updated in inner-loop, will be used for forward propagation. lower_model : Module, default=None The model whose parameters will be updated during upper-level optimization. upper_model : Module, default=None Upper model in a hierarchical model structure whose parameters will be updated with upper objective. meta_model : MetaModel or Module, default=None Model whose initial value will be optimized. If choose MAML method to optimize, any user-defined torch nn.Module could be used as long as the definition of forward() function meets the standard; but if choose other derived methods, internally defined both.utils.model.meta_model should be used for related additional modules. total_iters : int, default=60000 Total iterations of the experiment, used to set weight decay. Methods build_ll_problem_solver() Build LL-problem solver with both.lower_optimizer module, which will optimize lower model for further using in ul optimization procedure. Setting the value of parameters according to the selected method. Parameters: lower_loop(int) - The total number of iterations for lower gradient descent optimization. lower_objective_optimizer(Optimizer) - Optimizer of lower model, defined outside this module and will be used in LL optimization procedure. update_lower_model_with_step_num(int, default=0) - Whether to update lower model variables after ll optimization. Default value 0 means that lower model will maintain initial state after ll optimization process. If set this parameter to a positive integer k, then the lower model will save the updated results of step k of the ll optimization loop. Setting it when experiment doesn't have fine-tune stage. truncate_iter(int, default=0) - Specific parameter for Truncated Reverse AD method, defining number of iterations to truncate in the back propagation process during lower optimizing. acquire_max_loss(bool, default=False) - Specific parameter for IAPTT-GM method,if set True then will use IAPTT-GM method as lower optimization method. alpha_init(float, default=0.0) - Specify parameter for BDA method. The aggregation parameter for Bi-level descent aggregation method, where alpha \u2208 (0, 1) denotes the ratio of lower objective to upper objective during lower optimizing. learn_alpha(bool, default=False) - Specify parameter for BDA method to decide whether to initialize alpha as a hyper parameter. learn_alpha_itr(int, default=0) - Specify parameter for BDA method to specify whether to initialize alpha as a vector, of which every dimension's value is step-wise scale factor fot the optimization process. z_loop(int, default=5) - Specify parameter for BVFIM method. Num of steps to obtain a low ll problem value, i.e. optimize ll variable with ll problem. Regarded as T_z T_z in the paper. y_loop(int, default=5) - Specify parameter for BVFIM method. Num of steps to obtain a optimal ll variable under the ll problem value obtained after z_loop, i.e. optimize the updated ll variable with ul problem. Regarded as Regarded as T_y T_y in the paper. ll_l2_reg(float, default=0.1) - Specify parameter for BVFIM method. Weight of L2 regularization term in the value function of the regularizedLL problem. Referring to module both.upper_optimize.bvfim for more details. ul_l2_reg(float, default=0.01) - Specify parameter for BVFIM method. Weight of L2 regularization term in the value function of the regularized UL problem. Referring to module both.upper_optimize.bvfim for more details. ul_ln_reg(float, default=10.) - Specify parameter for BVFIM method. Weight of the log-barrier penalty term in the value function of the regularized UL problem. Referring to module both.upper_optimize.bvfim for more details. reg_decay(bool, default=True) - Specify parameter for BVFIM method. Whether to use weight decay coefficient of L2 regularization term and log-barrier penalty term. build_ul_problem_solver() Setting up UL optimization module. Select desired method through given parameters and set related experiment parameters. Details of parameter settings for each particular method are available in the specific method module of both.upper_optimize. Parameters: ul_optimizer(Optimizer) - Optimizer of upper model, defined outside this module and will be used in UL optimization procedure. method_igbm(str, default=None) - Specific parameter for Implicit method. The method used in the UL problem optimization. k(int, default=10) - Specific parameter for Implicit method. The maximum number of conjugate gradient iterations. tolerance(float, default=1e-10) - Specific parameter for Implicit method. End the method earlier when the norm of the residual is less than tolerance. r(float, default=1e-2) - Parameter for One-stage RAD method and used to adjust scalar epsilon. Value 0.01 of r is recommended for sufficiently accurate in the paper. Referring to module both.upper_optimize.onestage for more details. ll_l2_reg(float, default=0.1) - Specify parameter for BVFIM method. Weight of L2 regularization term in the value function of the regularizedLL problem. Referring to module both.upper_optimize.bvfim for more details. ul_l2_reg(float, default=0.01) - Specify parameter for BVFIM method. Weight of L2 regularization term in the value function of the regularized UL problem. Referring to module both.upper_optimize.bvfim for more details. ul_ln_reg(float, default=10.) - Specify parameter for BVFIM method. Weight of the log-barrier penalty term in the value function of the regularized UL problem. Referring to module both.upper_optimize.bvfim for more details. build_meta_problem_solver() Setting up meta-learning optimization module. Select desired method through given parameters and set set related experiment parameters. Note that among three methods MT-net, Warpgrad and L2F, only one can be used; while First-order and MSGD can be combined with others. Parameters: meta_optimizer(Optimizer) - The optimizer used to update initial values of meta model after an iteration. inner_loop(int, default=5) - Num of inner optimization steps. inner_learning_rate(float, default=0.01) - Step size for inner optimization. use_second_order(bool, default=True) - Optional argument, whether to calculate precise second-order gradients during inner-loop. learn_lr(bool, default=False) - Optional argument, whether to update inner learning rate during outer optimization, i.e. use MSGD method. use_t(bool, default=False) - Optional argument, whether to using T-layers during optimization,i.e. use MT-net method. use_warp(bool, default=False) - Optional argument, whether to using warp modules during optimization,i.e. use Warp-grad method. use_forget(bool, default=False) - Optional argument, whether to add attenuation to each layers, i.e. use L2F method. run_iter() Run an iteration with a data batch and updates the parameters of upper model or meta-model. Parameters: train_data_batch(Tensor) - A batch of train data,which is used during lower optimizing. train_target_batch(Tensor) - A batch of train target,which is used during lower optimizing. validate_data_batch(Tensor) - A batch of test data,which is used during upper optimizing. validate_target_batch(Tensor) - A batch of test target,which is used during upper optimizing. batch_size(int) - The number of training samples in each batch. current_iter(int) - The num of current iter. forward_with_whole_batch(bool, default=True) - Whether to feed in the whole data batch when doing forward propagation. When setting to False, each single data in the batch will be fed into model during this iteration. This useful for some experiment having special setting, like few-shot learning. Returns validation_loss(Tensor) - The value of validation loss value.","title":"both.optimizer"},{"location":"documents/boml.optimizer/#classbothoptimizer","text":"","title":"class&nbsp;&nbsp;BOTHOptimizer"},{"location":"documents/boml.optimizer/#description","text":"Wrapper for performing bi-level optimization and gradient-based initialization optimization BOTHOptimizer is the wrapper of Bi-Level Optimization(BLO) and Initialization Optimization(Initialization-based EGBR) process which builds LL, UL and Initialization problem solver with corresponding method modules and uses in training phase. The optimization process could also be done by using methods packages directly.","title":"Description"},{"location":"documents/boml.optimizer/#parameters","text":"method : str Define basic method for following training process, it should be included in ['MetaInit', 'MetaRepr']. 'MetaInit' type refers to meta-learning optimization strategy, including methods like 'MAML, FOMAML, TNet, WarpGrad, L2F'; 'MetaRepr' type refers to bi-level optimization strategy, includes methods like 'BDA, RHG, Truncated RHG, Implicit HG, Onestage, BVFIM, IAPTT-GM, BSG'. lower_method : str, default=None method chosen for solving LL problem, including ['Feature' ,'BVFIM']. upper_method : str, default=None Method chosen for solving UL problem, including ['Trad' ,'IAPTTGM', 'Implicit', 'BSG', 'Onestage', 'BVFIM']. lower_objective : callable, default=None An optimization problem which is considered as the constraint of UL problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\" Data used in the LL optimization phase. \"target\" Target used in the LL optimization phase. \"upper_model\" UL model of the bi-level model structure. \"lower_model\" LL model of the bi-level model structure. upper_objective : callable, default=None The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\" Data used in the UL optimization phase. \"target\" Target used in the UL optimization phase. \"upper_model\" Ul model of the bi-level model structure. \"lower_model\" LL model of the bi-level model structure. inner_objective : callable, default=None The inner loop optimization objective. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of inner objective. The state object contains the following: \"data\" Data used in inner optimization phase. \"target\" Target used in inner optimization phase. \"model\" Meta model to be updated. \"updated_weights\" Weights of model updated in inner-loop, will be used for forward propagation. outer_objective : callable, default=None The outer optimization objective. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of outer objective. The state object contains the following: \"data\" Data used in outer optimization phase. \"target\" Target used in outer optimization phase. \"model\" Meta model to be updated. \"updated_weights\" Weights of model updated in inner-loop, will be used for forward propagation. lower_model : Module, default=None The model whose parameters will be updated during upper-level optimization. upper_model : Module, default=None Upper model in a hierarchical model structure whose parameters will be updated with upper objective. meta_model : MetaModel or Module, default=None Model whose initial value will be optimized. If choose MAML method to optimize, any user-defined torch nn.Module could be used as long as the definition of forward() function meets the standard; but if choose other derived methods, internally defined both.utils.model.meta_model should be used for related additional modules. total_iters : int, default=60000 Total iterations of the experiment, used to set weight decay.","title":"Parameters"},{"location":"documents/boml.optimizer/#methods","text":"build_ll_problem_solver() Build LL-problem solver with both.lower_optimizer module, which will optimize lower model for further using in ul optimization procedure. Setting the value of parameters according to the selected method. Parameters: lower_loop(int) - The total number of iterations for lower gradient descent optimization. lower_objective_optimizer(Optimizer) - Optimizer of lower model, defined outside this module and will be used in LL optimization procedure. update_lower_model_with_step_num(int, default=0) - Whether to update lower model variables after ll optimization. Default value 0 means that lower model will maintain initial state after ll optimization process. If set this parameter to a positive integer k, then the lower model will save the updated results of step k of the ll optimization loop. Setting it when experiment doesn't have fine-tune stage. truncate_iter(int, default=0) - Specific parameter for Truncated Reverse AD method, defining number of iterations to truncate in the back propagation process during lower optimizing. acquire_max_loss(bool, default=False) - Specific parameter for IAPTT-GM method,if set True then will use IAPTT-GM method as lower optimization method. alpha_init(float, default=0.0) - Specify parameter for BDA method. The aggregation parameter for Bi-level descent aggregation method, where alpha \u2208 (0, 1) denotes the ratio of lower objective to upper objective during lower optimizing. learn_alpha(bool, default=False) - Specify parameter for BDA method to decide whether to initialize alpha as a hyper parameter. learn_alpha_itr(int, default=0) - Specify parameter for BDA method to specify whether to initialize alpha as a vector, of which every dimension's value is step-wise scale factor fot the optimization process. z_loop(int, default=5) - Specify parameter for BVFIM method. Num of steps to obtain a low ll problem value, i.e. optimize ll variable with ll problem. Regarded as T_z T_z in the paper. y_loop(int, default=5) - Specify parameter for BVFIM method. Num of steps to obtain a optimal ll variable under the ll problem value obtained after z_loop, i.e. optimize the updated ll variable with ul problem. Regarded as Regarded as T_y T_y in the paper. ll_l2_reg(float, default=0.1) - Specify parameter for BVFIM method. Weight of L2 regularization term in the value function of the regularizedLL problem. Referring to module both.upper_optimize.bvfim for more details. ul_l2_reg(float, default=0.01) - Specify parameter for BVFIM method. Weight of L2 regularization term in the value function of the regularized UL problem. Referring to module both.upper_optimize.bvfim for more details. ul_ln_reg(float, default=10.) - Specify parameter for BVFIM method. Weight of the log-barrier penalty term in the value function of the regularized UL problem. Referring to module both.upper_optimize.bvfim for more details. reg_decay(bool, default=True) - Specify parameter for BVFIM method. Whether to use weight decay coefficient of L2 regularization term and log-barrier penalty term. build_ul_problem_solver() Setting up UL optimization module. Select desired method through given parameters and set related experiment parameters. Details of parameter settings for each particular method are available in the specific method module of both.upper_optimize. Parameters: ul_optimizer(Optimizer) - Optimizer of upper model, defined outside this module and will be used in UL optimization procedure. method_igbm(str, default=None) - Specific parameter for Implicit method. The method used in the UL problem optimization. k(int, default=10) - Specific parameter for Implicit method. The maximum number of conjugate gradient iterations. tolerance(float, default=1e-10) - Specific parameter for Implicit method. End the method earlier when the norm of the residual is less than tolerance. r(float, default=1e-2) - Parameter for One-stage RAD method and used to adjust scalar epsilon. Value 0.01 of r is recommended for sufficiently accurate in the paper. Referring to module both.upper_optimize.onestage for more details. ll_l2_reg(float, default=0.1) - Specify parameter for BVFIM method. Weight of L2 regularization term in the value function of the regularizedLL problem. Referring to module both.upper_optimize.bvfim for more details. ul_l2_reg(float, default=0.01) - Specify parameter for BVFIM method. Weight of L2 regularization term in the value function of the regularized UL problem. Referring to module both.upper_optimize.bvfim for more details. ul_ln_reg(float, default=10.) - Specify parameter for BVFIM method. Weight of the log-barrier penalty term in the value function of the regularized UL problem. Referring to module both.upper_optimize.bvfim for more details. build_meta_problem_solver() Setting up meta-learning optimization module. Select desired method through given parameters and set set related experiment parameters. Note that among three methods MT-net, Warpgrad and L2F, only one can be used; while First-order and MSGD can be combined with others. Parameters: meta_optimizer(Optimizer) - The optimizer used to update initial values of meta model after an iteration. inner_loop(int, default=5) - Num of inner optimization steps. inner_learning_rate(float, default=0.01) - Step size for inner optimization. use_second_order(bool, default=True) - Optional argument, whether to calculate precise second-order gradients during inner-loop. learn_lr(bool, default=False) - Optional argument, whether to update inner learning rate during outer optimization, i.e. use MSGD method. use_t(bool, default=False) - Optional argument, whether to using T-layers during optimization,i.e. use MT-net method. use_warp(bool, default=False) - Optional argument, whether to using warp modules during optimization,i.e. use Warp-grad method. use_forget(bool, default=False) - Optional argument, whether to add attenuation to each layers, i.e. use L2F method. run_iter() Run an iteration with a data batch and updates the parameters of upper model or meta-model. Parameters: train_data_batch(Tensor) - A batch of train data,which is used during lower optimizing. train_target_batch(Tensor) - A batch of train target,which is used during lower optimizing. validate_data_batch(Tensor) - A batch of test data,which is used during upper optimizing. validate_target_batch(Tensor) - A batch of test target,which is used during upper optimizing. batch_size(int) - The number of training samples in each batch. current_iter(int) - The num of current iter. forward_with_whole_batch(bool, default=True) - Whether to feed in the whole data batch when doing forward propagation. When setting to False, each single data in the batch will be fed into model during this iteration. This useful for some experiment having special setting, like few-shot learning. Returns validation_loss(Tensor) - The value of validation loss value.","title":"Methods"},{"location":"documents/boml.utils/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"documents/boml.utils/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"documents/boml.utils/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"documents/boml.utils/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"documents/boml.ll_optimize/BVFIM/","text":"class BVFIM Description Lower model optimization procedure of Value-Function-based Interior-point Method Implements the ll problem optimization procedure of Value-Function Best- Response (VFBR) type BLO methods, named i-level Value-Function-basedInterior-point Method(BVFIM) [1] . The implemented lower level optimization procedure will optimize a wrapper of lower model for further using in the following upper level optimization. Parameters lower_objective : callable An optimization problem which is considered as the constraint of ll problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ll problem. The state object contains the following: \"data\"(Tensor) - Data used in the ll optimization phase. \"target\"(Tensor) - Target used in the ll optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective. upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) Data used in the ul optimization phase. \"target\"(Tensor) Target used in the ul optimization phase. \"upper_model\"(Module) Upper model of the bi-level model structure. \"lower_model\"(Module) Lower model of the bi-level model structure. z_loop (optional): int, default=5 Num of steps to obtain a low ll problem value, i.e. optimize ll variable with ll problem. Regarded as T_z T_z in the paper. y_loop (optional): int, default=5 Num of steps to obtain a optimal ll variable under the ll problem value obtained after z_loop, i.e. optimize the updated ll variable with ul problem. Regarded as Regarded as T_y T_y in the paper. z_l2_reg (optional): float, default=0.1 Weight of L2 regularization term in the value function of the regularized LL problem, which is \\displaystyle f_\\mu^*(x) = \\min_{y\\in\\mathbb{R}^n} f(x,y) + \\frac{\\mu_1}{2}\\|y\\|^2 + \\mu_2 \\displaystyle f_\\mu^*(x) = \\min_{y\\in\\mathbb{R}^n} f(x,y) + \\frac{\\mu_1}{2}\\|y\\|^2 + \\mu_2 . y_l2_reg (optional): float, default=0.01 Weight of L2 regularization term in the value function of the regularized UL problem, which is \\displaystyle \\varphi(x) = \\min_{y\\in\\mathbb{R}^n} F(x,y) + \\frac{\\theta}{2}\\|y\\|^2 - \\tau\\ln(f_\\mu^*(x)-f(x,y)) \\displaystyle \\varphi(x) = \\min_{y\\in\\mathbb{R}^n} F(x,y) + \\frac{\\theta}{2}\\|y\\|^2 - \\tau\\ln(f_\\mu^*(x)-f(x,y)) . y_ln_reg (optional): float, default=10. Weight of the log-barrier penalty term in the value function of the regularized UL problem, as y_l2_reg. Methods optimize() Execute the lower optimization procedure with training data samples using lower objective. The passed in wrapper of lower model will be updated. Parameters: train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, will be optimized in ll optimization procedure. auxiliary_opt(DifferentiableOptimizer) - Wrapper of ll optimizer encapsulated by module higher, will be used in ll optimization procedure. auxiliary_model_extra: Module auxiliary_opt_extra: Optimizer validate_data(Tensor) - The validation data used for ul problem optimization. Needed when using BDA method or IAPTT-GM method. validate_target(Tensor) - The labels of the samples in the validation data. Needed when using BDA method or IAPTT-GM method. reg_decay(float) - Weight decay coefficient of L2 regularization term and log-barrier penalty term. The value increases with the number of iterations References [1] R. Liu, X. Liu, X. Yuan, S. Zeng and J. Zhang, \"A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization\", in ICML, 2021.","title":"BVFIM"},{"location":"documents/boml.ll_optimize/BVFIM/#classbvfim","text":"","title":"class&nbsp;&nbsp;BVFIM"},{"location":"documents/boml.ll_optimize/BVFIM/#description","text":"Lower model optimization procedure of Value-Function-based Interior-point Method Implements the ll problem optimization procedure of Value-Function Best- Response (VFBR) type BLO methods, named i-level Value-Function-basedInterior-point Method(BVFIM) [1] . The implemented lower level optimization procedure will optimize a wrapper of lower model for further using in the following upper level optimization.","title":"Description"},{"location":"documents/boml.ll_optimize/BVFIM/#parameters","text":"lower_objective : callable An optimization problem which is considered as the constraint of ll problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ll problem. The state object contains the following: \"data\"(Tensor) - Data used in the ll optimization phase. \"target\"(Tensor) - Target used in the ll optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective. upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) Data used in the ul optimization phase. \"target\"(Tensor) Target used in the ul optimization phase. \"upper_model\"(Module) Upper model of the bi-level model structure. \"lower_model\"(Module) Lower model of the bi-level model structure. z_loop (optional): int, default=5 Num of steps to obtain a low ll problem value, i.e. optimize ll variable with ll problem. Regarded as T_z T_z in the paper. y_loop (optional): int, default=5 Num of steps to obtain a optimal ll variable under the ll problem value obtained after z_loop, i.e. optimize the updated ll variable with ul problem. Regarded as Regarded as T_y T_y in the paper. z_l2_reg (optional): float, default=0.1 Weight of L2 regularization term in the value function of the regularized LL problem, which is \\displaystyle f_\\mu^*(x) = \\min_{y\\in\\mathbb{R}^n} f(x,y) + \\frac{\\mu_1}{2}\\|y\\|^2 + \\mu_2 \\displaystyle f_\\mu^*(x) = \\min_{y\\in\\mathbb{R}^n} f(x,y) + \\frac{\\mu_1}{2}\\|y\\|^2 + \\mu_2 . y_l2_reg (optional): float, default=0.01 Weight of L2 regularization term in the value function of the regularized UL problem, which is \\displaystyle \\varphi(x) = \\min_{y\\in\\mathbb{R}^n} F(x,y) + \\frac{\\theta}{2}\\|y\\|^2 - \\tau\\ln(f_\\mu^*(x)-f(x,y)) \\displaystyle \\varphi(x) = \\min_{y\\in\\mathbb{R}^n} F(x,y) + \\frac{\\theta}{2}\\|y\\|^2 - \\tau\\ln(f_\\mu^*(x)-f(x,y)) . y_ln_reg (optional): float, default=10. Weight of the log-barrier penalty term in the value function of the regularized UL problem, as y_l2_reg.","title":"Parameters"},{"location":"documents/boml.ll_optimize/BVFIM/#methods","text":"optimize() Execute the lower optimization procedure with training data samples using lower objective. The passed in wrapper of lower model will be updated. Parameters: train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, will be optimized in ll optimization procedure. auxiliary_opt(DifferentiableOptimizer) - Wrapper of ll optimizer encapsulated by module higher, will be used in ll optimization procedure. auxiliary_model_extra: Module auxiliary_opt_extra: Optimizer validate_data(Tensor) - The validation data used for ul problem optimization. Needed when using BDA method or IAPTT-GM method. validate_target(Tensor) - The labels of the samples in the validation data. Needed when using BDA method or IAPTT-GM method. reg_decay(float) - Weight decay coefficient of L2 regularization term and log-barrier penalty term. The value increases with the number of iterations","title":"Methods"},{"location":"documents/boml.ll_optimize/BVFIM/#references","text":"[1] R. Liu, X. Liu, X. Yuan, S. Zeng and J. Zhang, \"A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization\", in ICML, 2021.","title":"References"},{"location":"documents/boml.ll_optimize/Feature/","text":"class Feature Description Lower level model optimization procedure Implements the ll problem optimization procedure of two explicit gradient based methods (EGBMs) with lower-level singleton (LLS) assumption, Reverse-mode AutoDiff method (RAD) [1] and Truncated RAD method (T-RAD) [2] , as well as two methods without LLS, Bi-level Descent Aggregation (BDA) [3] and Initialization Auxiliary and Pessimistic Trajectory Truncated Gradient method (IAPTT-GM) [4] . The implemented ll optimization procedure will optimize a wrapper of lower model for further using in the following ul optimization. Parameters lower_objective : callable An optimization problem which is considered as the constraint of ll problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ll problem. The state object contains the following: \"data\"(Tensor) - Data used in the ll optimization phase. \"target\"(Tensor) - Target used in the ll optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. lower_loop : int Updating iterations over ll optimization. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective. upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) Data used in the ul optimization phase. \"target\"(Tensor) Target used in the ul optimization phase. \"upper_model\"(Module) Upper model of the bi-level model structure. \"lower_model\"(Module) Lower model of the bi-level model structure. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization. acquire_max_loss (optional): bool, default=False Optional argument,if set True then will use IAPTT-GM method as ll optimization method. alpha (optional): float, default=0 The aggregation parameter for BDA method, where alpha \u2208 (0, 1) denotes the ratio of lower objective to upper objective during lower optimizing. truncate_iters (optional): int, default=0 Parameter for T-RAD method, defining number of iterations to truncate in the back propagation process during lower optimizing. lower_opt (optional): Optimizer, default=None The original optimizer of lower model. Methods optimize() Execute the lower optimization procedure with training data samples using lower objective. The passed in wrapper of lower model will be updated. Parameters: train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, will be optimized in ll optimization procedure. auxiliary_opt(DifferentiableOptimizer) - Wrapper of ll optimizer encapsulated by module higher, will be used in ll optimization procedure. validate_data(Tensor, optional, default=None) - The validation data used for ul problem optimization. Needed when using BDA method or IAPTT-GM method. validate_target(Tensor, optional, default=None) - The labels of the samples in the validation data. Needed when using BDA method or IAPTT-GM method. References [1] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil, \"Bilevel programming for hyperparameter optimization and meta-learning\", in ICML, 2018 [2] A. Shaban, C. Cheng, N. Hatch, and B. Boots, \"Truncated backpropagation for bilevel optimization\", in AISTATS, 2019. [3] R. Liu, P. Mu, X. Yuan, S. Zeng, and J. Zhang, \"A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton\", in ICML, 2020. [4] R. Liu, Y. Liu, S. Zeng, and J. Zhang, \"Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond\", in NeurIPS, 2021.","title":"Feature"},{"location":"documents/boml.ll_optimize/Feature/#classfeature","text":"","title":"class&nbsp;&nbsp;Feature"},{"location":"documents/boml.ll_optimize/Feature/#description","text":"Lower level model optimization procedure Implements the ll problem optimization procedure of two explicit gradient based methods (EGBMs) with lower-level singleton (LLS) assumption, Reverse-mode AutoDiff method (RAD) [1] and Truncated RAD method (T-RAD) [2] , as well as two methods without LLS, Bi-level Descent Aggregation (BDA) [3] and Initialization Auxiliary and Pessimistic Trajectory Truncated Gradient method (IAPTT-GM) [4] . The implemented ll optimization procedure will optimize a wrapper of lower model for further using in the following ul optimization.","title":"Description"},{"location":"documents/boml.ll_optimize/Feature/#parameters","text":"lower_objective : callable An optimization problem which is considered as the constraint of ll problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ll problem. The state object contains the following: \"data\"(Tensor) - Data used in the ll optimization phase. \"target\"(Tensor) - Target used in the ll optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. lower_loop : int Updating iterations over ll optimization. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective. upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) Data used in the ul optimization phase. \"target\"(Tensor) Target used in the ul optimization phase. \"upper_model\"(Module) Upper model of the bi-level model structure. \"lower_model\"(Module) Lower model of the bi-level model structure. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization. acquire_max_loss (optional): bool, default=False Optional argument,if set True then will use IAPTT-GM method as ll optimization method. alpha (optional): float, default=0 The aggregation parameter for BDA method, where alpha \u2208 (0, 1) denotes the ratio of lower objective to upper objective during lower optimizing. truncate_iters (optional): int, default=0 Parameter for T-RAD method, defining number of iterations to truncate in the back propagation process during lower optimizing. lower_opt (optional): Optimizer, default=None The original optimizer of lower model.","title":"Parameters"},{"location":"documents/boml.ll_optimize/Feature/#methods","text":"optimize() Execute the lower optimization procedure with training data samples using lower objective. The passed in wrapper of lower model will be updated. Parameters: train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, will be optimized in ll optimization procedure. auxiliary_opt(DifferentiableOptimizer) - Wrapper of ll optimizer encapsulated by module higher, will be used in ll optimization procedure. validate_data(Tensor, optional, default=None) - The validation data used for ul problem optimization. Needed when using BDA method or IAPTT-GM method. validate_target(Tensor, optional, default=None) - The labels of the samples in the validation data. Needed when using BDA method or IAPTT-GM method.","title":"Methods"},{"location":"documents/boml.ll_optimize/Feature/#references","text":"[1] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil, \"Bilevel programming for hyperparameter optimization and meta-learning\", in ICML, 2018 [2] A. Shaban, C. Cheng, N. Hatch, and B. Boots, \"Truncated backpropagation for bilevel optimization\", in AISTATS, 2019. [3] R. Liu, P. Mu, X. Yuan, S. Zeng, and J. Zhang, \"A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton\", in ICML, 2020. [4] R. Liu, Y. Liu, S. Zeng, and J. Zhang, \"Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond\", in NeurIPS, 2021.","title":"References"},{"location":"documents/boml.ll_optimize/LowerOptimize/","text":"boml.ll_optimize class LowerOptimize class BOMLLowerOptimizeFeature Description Lower level model optimization procedure with Implicit Gradient Based Methods # todo Implements the ll problem optimization procedure of two explicit gradient based methods (EGBMs) with lower-level singleton (LLS) assumption, Reverse-mode AutoDiff method (RAD) [1] and Truncated RAD method (T-RAD) [2] , as well as two methods without LLS, Bi-level descent aggregation (BDA) [3] and Initialization Auxiliary and Pessimistic Trajectory Truncated Gradient Method (IAPTT-GM) [4] . The implemented ll optimization procedure will optimize a wrapper of lower model for further using in the following ul optimization. Only one set of training data samples in the batch will be used in this procedure. Parameters lower_objective (callable) - An optimization problem which is considered as the constraint of ll problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ll problem. The state object contains the following: \"data\"(Tensor) - Data used in the lower optimization phase. \"target\"(Tensor) - Target used in the lower optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. lower_loop (int) - Updating iterations over lower level optimization. upper_model (Module) - Upper model in a hierarchical model structure whose parameters will be updated with upper objective. upper_objective (callable) - The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) Data used in the upper optimization phase. \"target\"(Tensor) Target used in the upper optimization phase. \"upper_model\"(Module) Upper model of the bi-level model structure. \"lower_model\"(Module) Lower model of the bi-level model structure. lower_model (Module) - Lower model in a hierarchical model structure whose parameters will be updated with lower objective during lower-level optimization. acquire_max_loss (bool, optional, default=False) - Optional argument,if set True then will use IAPTT-GM method as lower optimization method. alpha (float, optional, default=0) - The aggregation parameter for Bi-level Descent Aggregation method, where alpha \u2208 (0, 1) denotes the ratio of lower objective to upper objective during lower optimizing. truncate_iters (int, optional, default=0) - Parameter for Truncated Reverse method, defining number of iterations to truncate in the back propagation process during lower optimizing. lower_opt (Optimizer, optional, default=None) - The original optimizer of lower model. Methods optimize(train_data, train_target, auxiliary_model, auxiliary_opt, validate_data=None, validate_target=None) Execute the lower optimization procedure with one single set of training data samples in the batch using lower objective. The passed in wrapper of lower model. Parameters: train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, will be optimized in lower optimization procedure. auxiliary_opt(DifferentiableOptimizer) - Wrapper of lower optimizer encapsulated by module higher, will be used in lower optimization procedure. validate_data(Tensor, optional, default=None) - The validation data used for ul problem optimization. Needed when using BDA method or IAPTT-GM method. validate_target(Tensor, optional, default=None) - The labels of the samples in the validation data. Needed when using BDA method or IAPTT-GM method. References [1] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil, \"Bilevel programming for hyperparameter optimization and meta-learning\", in ICML, 2018 [2] A. Shaban, C. Cheng, N. Hatch, and B. Boots, \"Truncated backpropagation for bilevel optimization\", in AISTATS, 2019. [3] R. Liu, P. Mu, X. Yuan, S. Zeng, and J. Zhang, \"A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton\", in ICML, 2020. [4] R. Liu, Y. Liu, S. Zeng, and J. Zhang, \"Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond\", in NeurIPS, 2021. class BOMLLowerOptimizeBvfim selection: members: - adapt - clone -","title":"LowerOptimize"},{"location":"documents/boml.ll_optimize/LowerOptimize/#bomlll_optimize","text":"","title":"boml.ll_optimize"},{"location":"documents/boml.ll_optimize/LowerOptimize/#classloweroptimize","text":"","title":"class&nbsp;&nbsp;LowerOptimize"},{"location":"documents/boml.ll_optimize/LowerOptimize/#classbomlloweroptimizefeature","text":"","title":"class&nbsp;&nbsp;BOMLLowerOptimizeFeature"},{"location":"documents/boml.ll_optimize/LowerOptimize/#description","text":"Lower level model optimization procedure with Implicit Gradient Based Methods # todo Implements the ll problem optimization procedure of two explicit gradient based methods (EGBMs) with lower-level singleton (LLS) assumption, Reverse-mode AutoDiff method (RAD) [1] and Truncated RAD method (T-RAD) [2] , as well as two methods without LLS, Bi-level descent aggregation (BDA) [3] and Initialization Auxiliary and Pessimistic Trajectory Truncated Gradient Method (IAPTT-GM) [4] . The implemented ll optimization procedure will optimize a wrapper of lower model for further using in the following ul optimization. Only one set of training data samples in the batch will be used in this procedure.","title":"Description"},{"location":"documents/boml.ll_optimize/LowerOptimize/#parameters","text":"lower_objective (callable) - An optimization problem which is considered as the constraint of ll problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ll problem. The state object contains the following: \"data\"(Tensor) - Data used in the lower optimization phase. \"target\"(Tensor) - Target used in the lower optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. lower_loop (int) - Updating iterations over lower level optimization. upper_model (Module) - Upper model in a hierarchical model structure whose parameters will be updated with upper objective. upper_objective (callable) - The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) Data used in the upper optimization phase. \"target\"(Tensor) Target used in the upper optimization phase. \"upper_model\"(Module) Upper model of the bi-level model structure. \"lower_model\"(Module) Lower model of the bi-level model structure. lower_model (Module) - Lower model in a hierarchical model structure whose parameters will be updated with lower objective during lower-level optimization. acquire_max_loss (bool, optional, default=False) - Optional argument,if set True then will use IAPTT-GM method as lower optimization method. alpha (float, optional, default=0) - The aggregation parameter for Bi-level Descent Aggregation method, where alpha \u2208 (0, 1) denotes the ratio of lower objective to upper objective during lower optimizing. truncate_iters (int, optional, default=0) - Parameter for Truncated Reverse method, defining number of iterations to truncate in the back propagation process during lower optimizing. lower_opt (Optimizer, optional, default=None) - The original optimizer of lower model.","title":"Parameters"},{"location":"documents/boml.ll_optimize/LowerOptimize/#methods","text":"optimize(train_data, train_target, auxiliary_model, auxiliary_opt, validate_data=None, validate_target=None) Execute the lower optimization procedure with one single set of training data samples in the batch using lower objective. The passed in wrapper of lower model. Parameters: train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, will be optimized in lower optimization procedure. auxiliary_opt(DifferentiableOptimizer) - Wrapper of lower optimizer encapsulated by module higher, will be used in lower optimization procedure. validate_data(Tensor, optional, default=None) - The validation data used for ul problem optimization. Needed when using BDA method or IAPTT-GM method. validate_target(Tensor, optional, default=None) - The labels of the samples in the validation data. Needed when using BDA method or IAPTT-GM method.","title":"Methods"},{"location":"documents/boml.ll_optimize/LowerOptimize/#references","text":"[1] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil, \"Bilevel programming for hyperparameter optimization and meta-learning\", in ICML, 2018 [2] A. Shaban, C. Cheng, N. Hatch, and B. Boots, \"Truncated backpropagation for bilevel optimization\", in AISTATS, 2019. [3] R. Liu, P. Mu, X. Yuan, S. Zeng, and J. Zhang, \"A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton\", in ICML, 2020. [4] R. Liu, Y. Liu, S. Zeng, and J. Zhang, \"Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond\", in NeurIPS, 2021.","title":"References"},{"location":"documents/boml.ll_optimize/LowerOptimize/#classbomlloweroptimizebvfim","text":"selection: members: - adapt - clone -","title":"class&nbsp;&nbsp;BOMLLowerOptimizeBvfim"},{"location":"documents/boml.model/boml.model.backbone/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"both.model.backbone"},{"location":"documents/boml.model/boml.model.backbone/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"documents/boml.model/boml.model.backbone/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"documents/boml.model/boml.model.backbone/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"documents/boml.model/boml.model.meta_model/","text":"class MetaModel Description Special model used for initialization optimization with MAML and MAML based methods. Containing backbone model(CONV4, for example) and additional modules. Parameters backbone : Module Backbone model, could learn_lr : bool, default=False Whether to learning inner learning rate during outer optimization, i.e. use Meta-SGD method. meta_lr : float, default=0.1 Learning rate of inner optimization. use_t : bool, default=False Whether to add T-layers, i.e. use MT-net method. use_warp : bool, default=False Whether to add Warp-blocks, i.e. use Warp-grad method. num_warp_layers : int, default=1 Num of conv layers in one warp block. use_forget : bool, default=False Whether to add attenuator, i.e. use Learning-to-Forget method. enable_inner_loop_optimizable_bn_params : bool, default=False When use L2F method, whether to add the attenuation operation to the batch-norm modules.","title":"both.model.meta_model"},{"location":"documents/boml.model/boml.model.meta_model/#classmetamodel","text":"","title":"class&nbsp;&nbsp;MetaModel"},{"location":"documents/boml.model/boml.model.meta_model/#description","text":"Special model used for initialization optimization with MAML and MAML based methods. Containing backbone model(CONV4, for example) and additional modules.","title":"Description"},{"location":"documents/boml.model/boml.model.meta_model/#parameters","text":"backbone : Module Backbone model, could learn_lr : bool, default=False Whether to learning inner learning rate during outer optimization, i.e. use Meta-SGD method. meta_lr : float, default=0.1 Learning rate of inner optimization. use_t : bool, default=False Whether to add T-layers, i.e. use MT-net method. use_warp : bool, default=False Whether to add Warp-blocks, i.e. use Warp-grad method. num_warp_layers : int, default=1 Num of conv layers in one warp block. use_forget : bool, default=False Whether to add attenuator, i.e. use Learning-to-Forget method. enable_inner_loop_optimizable_bn_params : bool, default=False When use L2F method, whether to add the attenuation operation to the batch-norm modules.","title":"Parameters"},{"location":"documents/boml.ul_optimize/BSG/","text":"class BSG Description UL Variable Gradients Calculation with BSG Method Implements the ul problem optimization procedure of approximated Bilevel Stochastic Gradient method (BSG-1) [1] _, which approximates second-order ul gradient \\nabla F = \\nabla_xF - \\nabla_{xy}^2f(\\nabla_{yy}*2f)_{-1}\\nabla_yF \\nabla F = \\nabla_xF - \\nabla_{xy}^2f(\\nabla_{yy}*2f)_{-1}\\nabla_yF to first-order \\nabla_xF-\\frac{\\nabla_yf^\\top\\nabla_yF}{\\nabla_yf^\\top\\nabla_yf}\\nabla_xf \\nabla_xF-\\frac{\\nabla_yf^\\top\\nabla_yF}{\\nabla_yf^\\top\\nabla_yf}\\nabla_xf A wrapper of lower model that has been optimized in the lower optimization will be used in this procedure. Parameters upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_objective : callable An optimization problem which is considered as the constraint of ul problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) - Data used in the ul optimization phase. \"target\"(Tensor) - Target used in the ul optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization. lower_learning_rate : float Step size for ll optimization. Methods compute_gradients() Compute the grads of upper variable with validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables\u3002 If the validation data passed in is only single data of the batch (such as few-shot learning experiment), then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. Returns upper_loss(Tensor) - The loss value of upper objective. References [1] T. Giovannelli, G. Kent, L. N. Vicente, \"Bilevel stochastic methods for optimization and machine learning: Bilevel stochastic descent and DARTS\", in arxiv, 2021.","title":"BSG"},{"location":"documents/boml.ul_optimize/BSG/#classbsg","text":"","title":"class&nbsp;&nbsp;BSG"},{"location":"documents/boml.ul_optimize/BSG/#description","text":"UL Variable Gradients Calculation with BSG Method Implements the ul problem optimization procedure of approximated Bilevel Stochastic Gradient method (BSG-1) [1] _, which approximates second-order ul gradient \\nabla F = \\nabla_xF - \\nabla_{xy}^2f(\\nabla_{yy}*2f)_{-1}\\nabla_yF \\nabla F = \\nabla_xF - \\nabla_{xy}^2f(\\nabla_{yy}*2f)_{-1}\\nabla_yF to first-order \\nabla_xF-\\frac{\\nabla_yf^\\top\\nabla_yF}{\\nabla_yf^\\top\\nabla_yf}\\nabla_xf \\nabla_xF-\\frac{\\nabla_yf^\\top\\nabla_yF}{\\nabla_yf^\\top\\nabla_yf}\\nabla_xf A wrapper of lower model that has been optimized in the lower optimization will be used in this procedure.","title":"Description"},{"location":"documents/boml.ul_optimize/BSG/#parameters","text":"upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_objective : callable An optimization problem which is considered as the constraint of ul problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) - Data used in the ul optimization phase. \"target\"(Tensor) - Target used in the ul optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization. lower_learning_rate : float Step size for ll optimization.","title":"Parameters"},{"location":"documents/boml.ul_optimize/BSG/#methods","text":"compute_gradients() Compute the grads of upper variable with validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables\u3002 If the validation data passed in is only single data of the batch (such as few-shot learning experiment), then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. Returns upper_loss(Tensor) - The loss value of upper objective.","title":"Methods"},{"location":"documents/boml.ul_optimize/BSG/#references","text":"[1] T. Giovannelli, G. Kent, L. N. Vicente, \"Bilevel stochastic methods for optimization and machine learning: Bilevel stochastic descent and DARTS\", in arxiv, 2021.","title":"References"},{"location":"documents/boml.ul_optimize/BVFIM/","text":"class BVFIM Description Calculation of the gradient of the upper model variables with BVFIM method Implements the ul optimization procedure of Value-Function Best- Response (VFBR) type BLO methods, named i-level Value-Function-basedInterior-point Method(BVFIM) [1] . A wrapper of lower model that has been optimized in the lower optimization will be used in this procedure. Note that this ul optimization module should only use both.lower_optimize.Bvfim module to finish ll optimization procedure. Parameters upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_objective : callable An optimization problem which is considered as the constraint of ul problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) - Data used in the ul optimization phase. \"target\"(Tensor) - Target used in the ul optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. z_l2_reg (optional): float, default=0.1 Weight of L2 regularization term in the value function of the regularized LL problem, which is \\displaystyle f_\\mu^*(x) = \\min_{y\\in\\mathbb{R}^n} f(x,y) + \\frac{\\mu_1}{2}\\|y\\|^2 + \\mu_2 \\displaystyle f_\\mu^*(x) = \\min_{y\\in\\mathbb{R}^n} f(x,y) + \\frac{\\mu_1}{2}\\|y\\|^2 + \\mu_2 . y_l2_reg (optional): float, default=0.01 Weight of L2 regularization term in the value function of the regularized UL problem, which is \\displaystyle \\varphi(x) = \\min_{y\\in\\mathbb{R}^n} F(x,y) + \\frac{\\theta}{2}\\|y\\|^2 - \\tau\\ln(f_\\mu^*(x)-f(x,y)) \\displaystyle \\varphi(x) = \\min_{y\\in\\mathbb{R}^n} F(x,y) + \\frac{\\theta}{2}\\|y\\|^2 - \\tau\\ln(f_\\mu^*(x)-f(x,y)) . y_ln_reg (optional): float, default=10. Weight of the log-barrier penalty term in the value function of the regularized UL problem, as y_l2_reg. Methods compute_gradients() Compute the grads of upper variable with one single set of validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables with one single set of data samples in a batch. If batch size is larger than 1, then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. auxiliary_model_extra(Module) - train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. reg_decay(float) - Weight decay coefficient of L2 regularization term and log-barrier penalty term. The value increases with the number of iterations. Returns upper_loss(Tensor) - The loss value of upper objective. References [1] R. Liu, X. Liu, X. Yuan, S. Zeng and J. Zhang, \"A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization\", in ICML, 2021.","title":"BVFIM"},{"location":"documents/boml.ul_optimize/BVFIM/#classbvfim","text":"","title":"class&nbsp;&nbsp;BVFIM"},{"location":"documents/boml.ul_optimize/BVFIM/#description","text":"Calculation of the gradient of the upper model variables with BVFIM method Implements the ul optimization procedure of Value-Function Best- Response (VFBR) type BLO methods, named i-level Value-Function-basedInterior-point Method(BVFIM) [1] . A wrapper of lower model that has been optimized in the lower optimization will be used in this procedure. Note that this ul optimization module should only use both.lower_optimize.Bvfim module to finish ll optimization procedure.","title":"Description"},{"location":"documents/boml.ul_optimize/BVFIM/#parameters","text":"upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_objective : callable An optimization problem which is considered as the constraint of ul problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) - Data used in the ul optimization phase. \"target\"(Tensor) - Target used in the ul optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. z_l2_reg (optional): float, default=0.1 Weight of L2 regularization term in the value function of the regularized LL problem, which is \\displaystyle f_\\mu^*(x) = \\min_{y\\in\\mathbb{R}^n} f(x,y) + \\frac{\\mu_1}{2}\\|y\\|^2 + \\mu_2 \\displaystyle f_\\mu^*(x) = \\min_{y\\in\\mathbb{R}^n} f(x,y) + \\frac{\\mu_1}{2}\\|y\\|^2 + \\mu_2 . y_l2_reg (optional): float, default=0.01 Weight of L2 regularization term in the value function of the regularized UL problem, which is \\displaystyle \\varphi(x) = \\min_{y\\in\\mathbb{R}^n} F(x,y) + \\frac{\\theta}{2}\\|y\\|^2 - \\tau\\ln(f_\\mu^*(x)-f(x,y)) \\displaystyle \\varphi(x) = \\min_{y\\in\\mathbb{R}^n} F(x,y) + \\frac{\\theta}{2}\\|y\\|^2 - \\tau\\ln(f_\\mu^*(x)-f(x,y)) . y_ln_reg (optional): float, default=10. Weight of the log-barrier penalty term in the value function of the regularized UL problem, as y_l2_reg.","title":"Parameters"},{"location":"documents/boml.ul_optimize/BVFIM/#methods","text":"compute_gradients() Compute the grads of upper variable with one single set of validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables with one single set of data samples in a batch. If batch size is larger than 1, then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. auxiliary_model_extra(Module) - train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. reg_decay(float) - Weight decay coefficient of L2 regularization term and log-barrier penalty term. The value increases with the number of iterations. Returns upper_loss(Tensor) - The loss value of upper objective.","title":"Methods"},{"location":"documents/boml.ul_optimize/BVFIM/#references","text":"[1] R. Liu, X. Liu, X. Yuan, S. Zeng and J. Zhang, \"A Value-Function-based Interior-point Method for Non-convex Bi-level Optimization\", in ICML, 2021.","title":"References"},{"location":"documents/boml.ul_optimize/IAPTTGM/","text":"class Iapttgm Description UL Variable Gradients Calculation with IAPTT-GM Method Implements the UL problem optimization procedure of Initialization Auxiliary and Pessimistic Trajectory Truncated Gradient Method(IAPTT-GM) [1] , which designed for nal BLO in the absence of Lower-Level Convexity(LLC) hypothesis. A wrapper of lower model that has been optimized in the lower optimization will be used in this procedure. Parameters upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization. Methods compute_gradients() Compute the grads of upper variable with validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables\u3002 If the validation data passed in is only single data of the batch (such as few-shot learning experiment), then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. max_loss_iter(int) - The step of lower optimization loop which has the maximum loss value. The backpropagation trajectory shall be truncated here rather than the whole lower-loop. Returns upper_loss(Tensor) - The loss value of upper objective. References [1] R. Liu, Y. Liu, S. Zeng, J. Zhang, \"Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond\", in NeurIPS, 2021.","title":"IAPTTGM"},{"location":"documents/boml.ul_optimize/IAPTTGM/#classiapttgm","text":"","title":"class&nbsp;&nbsp;Iapttgm"},{"location":"documents/boml.ul_optimize/IAPTTGM/#description","text":"UL Variable Gradients Calculation with IAPTT-GM Method Implements the UL problem optimization procedure of Initialization Auxiliary and Pessimistic Trajectory Truncated Gradient Method(IAPTT-GM) [1] , which designed for nal BLO in the absence of Lower-Level Convexity(LLC) hypothesis. A wrapper of lower model that has been optimized in the lower optimization will be used in this procedure.","title":"Description"},{"location":"documents/boml.ul_optimize/IAPTTGM/#parameters","text":"upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization.","title":"Parameters"},{"location":"documents/boml.ul_optimize/IAPTTGM/#methods","text":"compute_gradients() Compute the grads of upper variable with validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables\u3002 If the validation data passed in is only single data of the batch (such as few-shot learning experiment), then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. max_loss_iter(int) - The step of lower optimization loop which has the maximum loss value. The backpropagation trajectory shall be truncated here rather than the whole lower-loop. Returns upper_loss(Tensor) - The loss value of upper objective.","title":"Methods"},{"location":"documents/boml.ul_optimize/IAPTTGM/#references","text":"[1] R. Liu, Y. Liu, S. Zeng, J. Zhang, \"Towards Gradient-based Bilevel Optimization with Non-convex Followers and Beyond\", in NeurIPS, 2021.","title":"References"},{"location":"documents/boml.ul_optimize/Implicit/","text":"class Implicit Description Calculation of the gradient of the upper model variables with Implicit Gradient Based Methods. Implements the ul optimization procedure of two implicit gradient based methods (IGBMs), linear system based method (LS) [1] and neumann series based method (NS) [2] . A wrapper of lower model which has been optimized in the ll optimization will be used in this procedure. Parameters upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_objective : callable An optimization problem which is considered as the constraint of ul problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) - Data used in the ul optimization phase. \"target\"(Tensor) - Target used in the ul optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization. lower_learning_rate : float Step size for ll optimization. k : int The maximum number of conjugate gradient iterations. method : {\"LS\", \"NS\"}, default=\"NS\" The method used in the ul problem optimization. tolerance : float, default=1e-10 End the method earlier when the norm of the residual is less than tolerance. Methods compute_gradients() Compute the grads of upper variable with validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables\u3002 If the validation data passed in is only single data of the batch (such as few-shot learning experiment), then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. Returns upper_loss(Tensor) - The loss value of upper objective. References [1] A. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine, \"Meta-learning with implicit gradients\", in NeurIPS, 2019. [2] J. Lorraine, P. Vicol, and D. Duvenaud, \"Optimizing millions of hyperparameters by implicit differentiation\", in AISTATS, 2020.","title":"Implicit"},{"location":"documents/boml.ul_optimize/Implicit/#classimplicit","text":"","title":"class&nbsp;&nbsp;Implicit"},{"location":"documents/boml.ul_optimize/Implicit/#description","text":"Calculation of the gradient of the upper model variables with Implicit Gradient Based Methods. Implements the ul optimization procedure of two implicit gradient based methods (IGBMs), linear system based method (LS) [1] and neumann series based method (NS) [2] . A wrapper of lower model which has been optimized in the ll optimization will be used in this procedure.","title":"Description"},{"location":"documents/boml.ul_optimize/Implicit/#parameters","text":"upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_objective : callable An optimization problem which is considered as the constraint of ul problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) - Data used in the ul optimization phase. \"target\"(Tensor) - Target used in the ul optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization. lower_learning_rate : float Step size for ll optimization. k : int The maximum number of conjugate gradient iterations. method : {\"LS\", \"NS\"}, default=\"NS\" The method used in the ul problem optimization. tolerance : float, default=1e-10 End the method earlier when the norm of the residual is less than tolerance.","title":"Parameters"},{"location":"documents/boml.ul_optimize/Implicit/#methods","text":"compute_gradients() Compute the grads of upper variable with validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables\u3002 If the validation data passed in is only single data of the batch (such as few-shot learning experiment), then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. Returns upper_loss(Tensor) - The loss value of upper objective.","title":"Methods"},{"location":"documents/boml.ul_optimize/Implicit/#references","text":"[1] A. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine, \"Meta-learning with implicit gradients\", in NeurIPS, 2019. [2] J. Lorraine, P. Vicol, and D. Duvenaud, \"Optimizing millions of hyperparameters by implicit differentiation\", in AISTATS, 2020.","title":"References"},{"location":"documents/boml.ul_optimize/Onestage/","text":"class Onestage Description Calculation of the gradient of the upper model variables with DARTS method Implements the ul optimization procedure of DARTS [1] , a first order approximation method which is free of both second-order derivatives and matrix-vector products. A wrapper of lower model that has been optimized in the ll optimization will be used in this procedure. Parameters upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_objective : callable An optimization problem which is considered as the constraint of ul problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) - Data used in the ul optimization phase. \"target\"(Tensor) - Target used in the ul optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization. lower_learning_rate : float Step size for ll optimization. r (optional): float, default=1e-2 Parameter to adjust scalar \\epsilon \\epsilon as: \\epsilon = 0.01/\\|\\nabla_{w'}\\mathcal L_{val}(w',\\alpha)\\|_2 \\epsilon = 0.01/\\|\\nabla_{w'}\\mathcal L_{val}(w',\\alpha)\\|_2 , and \\epsilon \\epsilon is used as: w^\\pm = w \\pm \\epsilon\\nabla_{w'}\\mathcal L_{val}(w',\\alpha) w^\\pm = w \\pm \\epsilon\\nabla_{w'}\\mathcal L_{val}(w',\\alpha) . Value 0.01 of r is recommended for sufficiently accurate in the paper. Methods compute_gradients() Compute the grads of upper variable with validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables\u3002 If the validation data passed in is only single data of the batch (such as few-shot learning experiment), then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. Returns upper_loss(Tensor) - The loss value of upper objective. References [1] H. Liu, K. Simonyan, Y. Yang, \"DARTS: Differentiable Architecture Search\", in ICLR, 2019.","title":"Onestage"},{"location":"documents/boml.ul_optimize/Onestage/#classonestage","text":"","title":"class&nbsp;&nbsp;Onestage"},{"location":"documents/boml.ul_optimize/Onestage/#description","text":"Calculation of the gradient of the upper model variables with DARTS method Implements the ul optimization procedure of DARTS [1] , a first order approximation method which is free of both second-order derivatives and matrix-vector products. A wrapper of lower model that has been optimized in the ll optimization will be used in this procedure.","title":"Description"},{"location":"documents/boml.ul_optimize/Onestage/#parameters","text":"upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_objective : callable An optimization problem which is considered as the constraint of ul problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of ul problem. The state object contains the following: \"data\"(Tensor) - Data used in the ul optimization phase. \"target\"(Tensor) - Target used in the ul optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization. lower_learning_rate : float Step size for ll optimization. r (optional): float, default=1e-2 Parameter to adjust scalar \\epsilon \\epsilon as: \\epsilon = 0.01/\\|\\nabla_{w'}\\mathcal L_{val}(w',\\alpha)\\|_2 \\epsilon = 0.01/\\|\\nabla_{w'}\\mathcal L_{val}(w',\\alpha)\\|_2 , and \\epsilon \\epsilon is used as: w^\\pm = w \\pm \\epsilon\\nabla_{w'}\\mathcal L_{val}(w',\\alpha) w^\\pm = w \\pm \\epsilon\\nabla_{w'}\\mathcal L_{val}(w',\\alpha) . Value 0.01 of r is recommended for sufficiently accurate in the paper.","title":"Parameters"},{"location":"documents/boml.ul_optimize/Onestage/#methods","text":"compute_gradients() Compute the grads of upper variable with validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables\u3002 If the validation data passed in is only single data of the batch (such as few-shot learning experiment), then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. train_data(Tensor) - The training data used for ll problem optimization. train_target(Tensor) - The labels of the samples in the train data. Returns upper_loss(Tensor) - The loss value of upper objective.","title":"Methods"},{"location":"documents/boml.ul_optimize/Onestage/#references","text":"[1] H. Liu, K. Simonyan, Y. Yang, \"DARTS: Differentiable Architecture Search\", in ICLR, 2019.","title":"References"},{"location":"documents/boml.ul_optimize/RAD/","text":"class RAD Description UL Variable Gradients Calculation with Reverse-mode AD Implements the ul optimization procedure with Reverse-mode Auto Diff method [1] . A wrapper of lower model that has been optimized in the lower optimization will be used in this procedure. Parameters upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization. Methods compute_gradients() Compute the grads of upper variable with validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables\u3002 If the validation data passed in is only single data of the batch (such as few-shot learning experiment), then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. Returns upper_loss(Tensor) - The loss value of upper objective. References [1] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil, \"Bilevel programming for hyperparameter optimization and meta-learning\", in ICML, 2018.","title":"RAD"},{"location":"documents/boml.ul_optimize/RAD/#classrad","text":"","title":"class&nbsp;&nbsp;RAD"},{"location":"documents/boml.ul_optimize/RAD/#description","text":"UL Variable Gradients Calculation with Reverse-mode AD Implements the ul optimization procedure with Reverse-mode Auto Diff method [1] . A wrapper of lower model that has been optimized in the lower optimization will be used in this procedure.","title":"Description"},{"location":"documents/boml.ul_optimize/RAD/#parameters","text":"upper_objective : callable The main optimization problem in a hierarchical optimization problem. Callable with signature callable(state). Defined based on modeling of the specific problem that need to be solved. Computing the loss of upper problem. The state object contains the following: \"data\"(Tensor) - Data used in the upper optimization phase. \"target\"(Tensor) - Target used in the upper optimization phase. \"upper_model\"(Module) - Upper model of the bi-level model structure. \"lower_model\"(Module) - Lower model of the bi-level model structure. upper_model : Module Upper model in a hierarchical model structure whose parameters will be updated with upper objective and trained lower model. lower_model : Module Lower model in a hierarchical model structure whose parameters will be updated with lower objective during ll optimization.","title":"Parameters"},{"location":"documents/boml.ul_optimize/RAD/#methods","text":"compute_gradients() Compute the grads of upper variable with validation data samples in the batch using upper objective. The grads will be saved in the passed in upper model. Note that the implemented ul optimization procedure will only compute the grads of upper variables\u3002 If the validation data passed in is only single data of the batch (such as few-shot learning experiment), then compute_gradients() function should be called repeatedly to accumulate the grads of upper variables for the whole batch. After that the update operation of upper variables needs to be done outside this module. Parameters: validate_data(Tensor) - The validation data used for ul problem optimization. validate_target(Tensor) - The labels of the samples in the validation data. auxiliary_model(_MonkeyPatchBase) - Wrapper of lower model encapsulated by module higher, has been optimized in ll optimization phase. Returns upper_loss(Tensor) - The loss value of upper objective.","title":"Methods"},{"location":"documents/boml.ul_optimize/RAD/#references","text":"[1] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil, \"Bilevel programming for hyperparameter optimization and meta-learning\", in ICML, 2018.","title":"References"},{"location":"documents/boml.ul_optimize/UpperGrad/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"UpperGrad"},{"location":"documents/boml.ul_optimize/UpperGrad/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"documents/boml.ul_optimize/UpperGrad/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"documents/boml.ul_optimize/UpperGrad/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"examples/applications/","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"applications"},{"location":"examples/applications/#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"examples/applications/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"examples/applications/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"examples/methods/","text":"Welcome to MkDocs BDA RAD TRAD OneStage-RAD LS NS BVFIM IAPTT-GM MAML MSGD MT-net Warp-grad L2F","title":"methods"},{"location":"examples/methods/#welcome-to-mkdocs","text":"","title":"Welcome to MkDocs"},{"location":"examples/methods/#bda","text":"","title":"BDA"},{"location":"examples/methods/#rad","text":"","title":"RAD"},{"location":"examples/methods/#trad","text":"","title":"TRAD"},{"location":"examples/methods/#onestage-rad","text":"","title":"OneStage-RAD"},{"location":"examples/methods/#ls","text":"","title":"LS"},{"location":"examples/methods/#ns","text":"","title":"NS"},{"location":"examples/methods/#bvfim","text":"","title":"BVFIM"},{"location":"examples/methods/#iaptt-gm","text":"","title":"IAPTT-GM"},{"location":"examples/methods/#maml","text":"","title":"MAML"},{"location":"examples/methods/#msgd","text":"","title":"MSGD"},{"location":"examples/methods/#mt-net","text":"","title":"MT-net"},{"location":"examples/methods/#warp-grad","text":"","title":"Warp-grad"},{"location":"examples/methods/#l2f","text":"","title":"L2F"},{"location":"examples/practical_use/","text":"","title":"practical use"}]}